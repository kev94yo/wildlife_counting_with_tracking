{"cells":[{"metadata":{},"cell_type":"markdown","source":["## Dataset derived from [iWildCam 2021 - Starter Notebook](https://www.kaggle.com/nayuts/iwildcam-2021-starter-notebook#Explanation-for-metadata-file)."]},{"metadata":{},"cell_type":"markdown","source":["I created a 256x256 image dataset for prototyping. Using the MegaDetector detection results for each image, I cropped out the location of the animal. Images with no detection results were resized as is. After the cropping process, the cropped out images were indexed for easy use in learning. A summary of the process is shown below."]},{"metadata":{},"cell_type":"markdown","source":["# Preparation"]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["import json\n","import os\n","import pickle\n","\n","import cv2\n","import numpy as np \n","import pandas as pd \n","from PIL import Image, ImageFile, ImageFont, ImageDraw"],"execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["TRAIN_PATH = \"./data/train/\"\n","TEST_PATH = \"./data/test/\"\n","ANNOTATIONS_PATH = \"./data/metadata/\"\n","\n","CROPED_TRAIN_PATH = \"./data/crop_train/\"\n","CROPED_TEST_PATH = \"./data/crop_test/\"\n","\n","threshold = 0.9"],"execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["with open(ANNOTATIONS_PATH + 'iwildcam2021_megadetector_results.json', encoding='utf-8') as json_file:\n","    megadetector_results =json.load(json_file)"],"execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# os.mkdir(CROPED_TRAIN_PATH)\n","# os.mkdir(CROPED_TEST_PATH)"],"execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["with open('./data/metadata/iwildcam2021_train_annotations.json', encoding='utf-8') as json_file:\n","    train_annotations =json.load(json_file)"],"execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["annotations = megadetector_results[\"images\"]"],"execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def get_crop_area(bbox, image_size):\n","    x1, y1,w_box, h_box = bbox\n","    ymin,xmin,ymax, xmax = y1, x1, y1 + h_box, x1 + w_box\n","    area = (xmin * image_size[0], ymin * image_size[1], \n","            xmax * image_size[0], ymax * image_size[1])\n","    return area"],"execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["img_ids_train = []\n","img_idx_train = []\n","img_ids_test = []\n","img_idx_test = []\n","\n","x_tot_list,x2_tot_list = [],[]"],"execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def save_image(img, img_id, idx, is_train):  \n","    if is_train:\n","        img.save( CROPED_TRAIN_PATH + f\"{img_id}_{idx}.jpg\", format=\"jpeg\")\n","        img_ids_train.append(f\"{img_id}\")\n","        img_idx_train.append(idx)\n","    else:\n","        img.save( CROPED_TEST_PATH + f\"{img_id}_{idx}.jpg\", format=\"jpeg\")\n","        img_ids_test.append(f\"{img_id}\")\n","        img_idx_test.append(idx)"],"execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def calc_x_and_x2_tot(img):\n","    \n","    img = np.array(img, dtype=np.uint8)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    img = np.transpose(img,(2,0,1))\n","            \n","    return (img/255.0).reshape(-1,3).mean(0), ((img/255.0)**2).reshape(-1,3).mean(0)"],"execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["## main processing moduleâ†“"]},{"metadata":{"trusted":true},"cell_type":"code","source":["def convert_images(annotation):\n","\n","    size = (256,256)\n","    img_id = annotation[\"id\"]\n","    is_train = True \n","    \n","    try:\n","        detections = annotation[\"detections\"]\n","    except:\n","        print(f\"Passed {img_id}. There are no detection data.\")\n","        return\n","    \n","    path_for_train = TRAIN_PATH + annotation[\"id\"] + \".jpg\"\n","    path_for_test = TEST_PATH + annotation[\"id\"] + \".jpg\"\n","    \n","    if os.path.exists(path_for_train):\n","        file_path = path_for_train\n","    elif os.path.exists(path_for_test):\n","        file_path = path_for_test\n","        is_train = False\n","    else:\n","        print(f\"Passed {img_id}. There are no data.\")\n","        return\n","    \n","    \n","    try:      \n","        img = Image.open(file_path)\n","    except:\n","        print(f\"Passed {img_id}. Fail to open image.\")\n","        print(f\"pass {file_path}.\")\n","        return\n","    \n","    for i, detection in enumerate(detections, 1):\n","        \n","        if detection[\"conf\"] < threshold:\n","            continue\n","\n","        if detection[\"category\"] != \"1\":\n","            continue\n","            \n","        if len(detection) == 0:\n","            img = img.resize(size)\n","            save_image(img, img_id, 0, is_train)\n","            \n","            x_tot, x2_tot = calc_mean_and_var(img)\n","            x_tot_list.append(x_tot)\n","            x2_tot_list.append(x2_tot)\n","            \n","        else:\n","            crop_area = get_crop_area(detection[\"bbox\"], img.size)\n","            img_cropped = img.crop(crop_area).resize(size)\n","            save_image(img_cropped, img_id, i, is_train)\n","        \n","            x_tot, x2_tot = calc_x_and_x2_tot(img_cropped)\n","            x_tot_list.append(x_tot)\n","            x2_tot_list.append(x2_tot)"],"execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Cropping\n","\n","Crop by calling convert_images fuction."]},{"metadata":{"trusted":true},"cell_type":"code","source":["for annotation in annotations:\n","    convert_images(annotation)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Passed 8e7320aa-21bc-11ea-a13a-137349068a90. Fail to open image.\npass ./data/train/8e7320aa-21bc-11ea-a13a-137349068a90.jpg.\n"]}]},{"metadata":{},"cell_type":"markdown","source":["The mean and variance for standardization."]},{"metadata":{"trusted":true},"cell_type":"code","source":["#image stats\n","img_avr =  np.array(x_tot_list).mean(0)\n","img_std =  np.sqrt(np.array(x2_tot_list).mean(0) - img_avr**2)\n","print('mean:',img_avr, ', std:', img_std)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["mean: [0.37087523 0.370876   0.3708759 ] , std: [0.21022698 0.21022713 0.21022706]\n"]}]},{"metadata":{},"cell_type":"markdown","source":["## zipping"]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":["!zip croped_images_train -r ./croped_images_train\n","!zip croped_images_test -r ./croped_images_test"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["!rm -r ./croped_images_train\n","!rm -r ./croped_images_test"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Creating csv file for reference"]},{"metadata":{"trusted":true},"cell_type":"code","source":["cropped_train = {\"id\": img_ids_train, \"idx\":img_idx_train}\n","df_cropped_train = pd.DataFrame(cropped_train)\n","df_train_annotation = pd.DataFrame(train_annotations[\"annotations\"])"],"execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["df_cropped_train = df_cropped_train.merge(\n","    df_train_annotation[[\"image_id\", \"category_id\"]], \n","    left_on='id', right_on='image_id')[[\"id\", \"idx\", \"category_id\"]]"],"execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["df_cropped_train.head()"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                     id  idx  category_id\n","0  905a3c8c-21bc-11ea-a13a-137349068a90    1          374\n","1  905a3c8c-21bc-11ea-a13a-137349068a90    1          374\n","2  905a4416-21bc-11ea-a13a-137349068a90    1           97\n","3  905a4416-21bc-11ea-a13a-137349068a90    2           97\n","4  905a4416-21bc-11ea-a13a-137349068a90    3           97"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>idx</th>\n      <th>category_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>905a3c8c-21bc-11ea-a13a-137349068a90</td>\n      <td>1</td>\n      <td>374</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>905a3c8c-21bc-11ea-a13a-137349068a90</td>\n      <td>1</td>\n      <td>374</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>905a4416-21bc-11ea-a13a-137349068a90</td>\n      <td>1</td>\n      <td>97</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>905a4416-21bc-11ea-a13a-137349068a90</td>\n      <td>2</td>\n      <td>97</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>905a4416-21bc-11ea-a13a-137349068a90</td>\n      <td>3</td>\n      <td>97</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":18}]},{"metadata":{"trusted":true},"cell_type":"code","source":["cropped_test = {\"id\": img_ids_test, \"idx\":img_idx_test}\n","df_cropped_test = pd.DataFrame(cropped_test)"],"execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["df_cropped_test.head()"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                     id  idx\n","0  915879a0-21bc-11ea-a13a-137349068a90    1\n","1  91588116-21bc-11ea-a13a-137349068a90    1\n","2  9158a2f4-21bc-11ea-a13a-137349068a90    1\n","3  9158aaa6-21bc-11ea-a13a-137349068a90    1\n","4  9158f1a0-21bc-11ea-a13a-137349068a90    1"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>915879a0-21bc-11ea-a13a-137349068a90</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>91588116-21bc-11ea-a13a-137349068a90</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9158a2f4-21bc-11ea-a13a-137349068a90</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9158aaa6-21bc-11ea-a13a-137349068a90</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9158f1a0-21bc-11ea-a13a-137349068a90</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":20}]},{"metadata":{"trusted":true},"cell_type":"code","source":["df_cropped_train.to_csv(\"./data/cropped_train.csv\", index=False)\n","df_cropped_test.to_csv(\"./data/cropped_test.csv\", index=False)"],"execution_count":21,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.8.5","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}